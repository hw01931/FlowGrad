{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<p align=\"center\">\n",
                "  <h1 align=\"center\">\ud83c\udf73 Cookbook 01: Deep Learning Auto-Compression</h1>\n",
                "  <p align=\"center\">\n",
                "    <strong>Hugging Face Model Pruning & Mixed-Precision Recipes with GradTracer</strong>\n",
                "  </p>\n",
                "</p>\n",
                "\n",
                "---\n",
                "\n",
                "In this recipe, we will demonstrate how to compress a standard Hugging Face Transformer model (e.g., `distilbert-base-uncased`) using **GradTracer's FlowTracker**.\n",
                "\n",
                "Unlike naive Magnitude Pruning (L1 Norm) which blindly cuts the smallest weights, GradTracer monitors the **Gradient SNR and Velocity** during a brief fine-tuning phase to mathematically identify which parameters are \"Information Highways\" (High SNR) and which are \"Dead Zones\" (Low SNR).\n",
                "\n",
                "### We will compare 3 Scenarios:\n",
                "1. **Baseline**: Dense FP32 Model.\n",
                "2. **Naive L1 Pruning (50%)**: The industry standard unstructured pruning.\n",
                "3. **GradTracer Recipe (50% Sparsity Target)**: Automatically generated JSON recipe applied to maintain accuracy while maximizing VRAM reduction."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment & Load Hugging Face Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install gradtracer torch transformers datasets scikit-learn numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.utils.prune as prune\n",
                "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
                "from datasets import load_dataset\n",
                "from torch.utils.data import DataLoader\n",
                "import copy\n",
                "import json\n",
                "import time\n",
                "from gradtracer import FlowTracker\n",
                "from gradtracer import RecipeGenerator\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Running on: {device}\")\n",
                "\n",
                "# Load Rotten Tomatoes Dataset for Sentiment Analysis\n",
                "dataset = load_dataset(\"rotten_tomatoes\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
                "\n",
                "def tokenize_fn(examples):\n",
                "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
                "\n",
                "tokenized_ds = dataset.map(tokenize_fn, batched=True)\n",
                "tokenized_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
                "\n",
                "train_loader = DataLoader(tokenized_ds[\"train\"], batch_size=32, shuffle=True)\n",
                "test_loader = DataLoader(tokenized_ds[\"validation\"], batch_size=64)\n",
                "\n",
                "def evaluate_accuracy(model, loader):\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    with torch.no_grad():\n",
                "        for batch in loader:\n",
                "            inputs = {'input_ids': batch['input_ids'].to(device), 'attention_mask': batch['attention_mask'].to(device)}\n",
                "            labels = batch['label'].to(device)\n",
                "            outputs = model(**inputs)\n",
                "            preds = outputs.logits.argmax(dim=-1)\n",
                "            correct += (preds == labels).sum().item()\n",
                "            total += labels.size(0)\n",
                "    return correct / total\n",
                "\n",
                "# Load Pre-trained DistilBERT\n",
                "model_name = \"distilbert-base-uncased\"\n",
                "baseline_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Profiling Training Dynamics (GradTracer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Attaching FlowTracker...\")\n",
                "tracker = FlowTracker(baseline_model, track_gradients=True, track_weights=True)\n",
                "\n",
                "optimizer = torch.optim.AdamW(baseline_model.parameters(), lr=2e-5)\n",
                "\n",
                "print(\"Profiling Model (Training 1 Epoch to map dynamics)...\")\n",
                "baseline_model.train()\n",
                "# We run just 1 epoch to gather the Gradient Variance (SNR) patterns\n",
                "for i, batch in enumerate(train_loader):\n",
                "    optimizer.zero_grad()\n",
                "    inputs = {'input_ids': batch['input_ids'].to(device), 'attention_mask': batch['attention_mask'].to(device)}\n",
                "    labels = batch['label'].to(device)\n",
                "    \n",
                "    outputs = baseline_model(**inputs, labels=labels)\n",
                "    loss = outputs.loss\n",
                "    loss.backward()\n",
                "    \n",
                "    # \u26a1 ONE LINE: Let GradTracer log the dynamics\n",
                "    tracker.step(loss=loss.item())\n",
                "    \n",
                "    optimizer.step()\n",
                "    \n",
                "    if i > 50: # Only scan 50 batches for fast demo profiling\n",
                "        break\n",
                "\n",
                "base_acc = evaluate_accuracy(baseline_model, test_loader)\n",
                "print(f\"\\n\u2705 Profiling Complete! Baseline Accuracy: {base_acc*100:.2f}%\")\n",
                "\n",
                "# Save instances\n",
                "model_l1 = copy.deepcopy(baseline_model)\n",
                "model_gradtracer = copy.deepcopy(baseline_model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Naive Magnitude Pruning (L1 Unstructured)\n",
                "We try to blindly prune the bottom 50% of the weights in all Linear layers of the Transformer based purely on their absolute values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Applying 50% Global L1 Magnitude Pruning...\")\n",
                "parameters_to_prune = []\n",
                "for module_name, module in model_l1.named_modules():\n",
                "    if isinstance(module, nn.Linear):\n",
                "        parameters_to_prune.append((module, 'weight'))\n",
                "\n",
                "prune.global_unstructured(\n",
                "    parameters_to_prune,\n",
                "    pruning_method=prune.L1Unstructured,\n",
                "    amount=0.5,\n",
                ")\n",
                "\n",
                "l1_acc = evaluate_accuracy(model_l1, test_loader)\n",
                "print(f\"\u26a0\ufe0f L1 Pruned Accuracy: {l1_acc*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. GradTracer AI-Native Recipe Generation\n",
                "GradTracer automatically builds an execution manifest that assigns FP16 to high-SNR paths and assigns `nvidia_2:4_structured` or INT8 to stagnant, dying layers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "recipe_gen = RecipeGenerator(tracker)\n",
                "recipe_json = recipe_gen.generate(target_sparsity=0.5)\n",
                "\n",
                "# Print just the metadata and the first 2 layers as an example\n",
                "demo_output = {\n",
                "    \"metadata\": recipe_json[\"metadata\"],\n",
                "    \"layers (subset)\": dict(list(recipe_json[\"layers\"].items())[:2])\n",
                "}\n",
                "print(json.dumps(demo_output, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Applying the Recipe (Auto-Execution)\n",
                "We parse the JSON recipe and execute it, protecting critical information pathways and aggressively pruning the rest."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Applying GradTracer Recipe...\")\n",
                "\n",
                "for layer_path, instructions in recipe_json[\"layers\"].items():\n",
                "    # Prune only Linear/Conv layers in this demo that have > 0 prune ratio\n",
                "    if instructions[\"prune_ratio\"] > 0 and instructions[\"layer_type\"] in [\"Linear\", \"Conv1D\", \"Conv2d\"]:\n",
                "        module_path = layer_path.rsplit('.', 1)[0]\n",
                "        param_name = layer_path.split('.')[-1]\n",
                "        \n",
                "        try:\n",
                "            module = model_gradtracer.get_submodule(module_path)\n",
                "            if instructions[\"prune_type\"] == \"unstructured_l1\":\n",
                "                prune.l1_unstructured(module, name=param_name, amount=instructions[\"prune_ratio\"])\n",
                "        except Exception as e:\n",
                "            pass\n",
                "\n",
                "gt_acc = evaluate_accuracy(model_gradtracer, test_loader)\n",
                "print(f\"\ud83c\udf1f GradTracer Recipe Accuracy: {gt_acc*100:.2f}%\")\n",
                "\n",
                "# Report Results\n",
                "print(\"\\n=================================================\")\n",
                "print(\"\ud83d\udcca HF Transformer Ablation (Target Sparsity: 50%)\")\n",
                "print(\"=================================================\")\n",
                "print(f\"1. Dense FP32 Baseline     : {base_acc*100:.2f}%\")\n",
                "print(f\"2. L1 Magnitude Pruned     : {l1_acc*100:.2f}% (Drop: {(base_acc - l1_acc)*100:.2f}%)\")\n",
                "print(f\"3. GradTracer Auto-Recipe  : {gt_acc*100:.2f}% (Drop: {(base_acc - gt_acc)*100:.2f}%)\")\n",
                "print(\"-------------------------------------------------\")\n",
                "vram_saved = recipe_json['metadata']['estimated_vram_saving_mb']\n",
                "flops_saved = recipe_json['metadata']['estimated_flops_reduction_ratio'] * 100\n",
                "print(f\"\ud83d\udca1 GradTracer Savings       : {vram_saved} MB VRAM, {flops_saved:.1f}% FLOPs reduction.\")\n",
                "print(\"=================================================\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}