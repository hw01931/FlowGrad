{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŒŠ FlowGrad v0.3 â€” Complete Demo\n",
                "\n",
                "**One-line Training Diagnostics, Feature Engineering & Model Compression for DL, ML, and RecSys**\n",
                "\n",
                "This notebook demonstrates all FlowGrad features with real datasets (no downloads needed):\n",
                "\n",
                "| Section | Feature | Dataset |\n",
                "|---|---|---|\n",
                "| 1 | sklearn GradientBoosting tracking | Breast Cancer |\n",
                "| 2 | RandomForest per-tree analysis | Breast Cancer |\n",
                "| 3 | XGBoost round-by-round diagnostics | California Housing |\n",
                "| 4 | **Feature Engineering** â€” interactions, suggestions with VIF | California Housing |\n",
                "| 5 | **Before/After Comparison** â€” prove features actually help | California Housing |\n",
                "| 6 | PyTorch DL layer health tracking | Breast Cancer |\n",
                "| 7 | **Model Compression** â€” auto pruning search & layer sensitivity â­ | Breast Cancer |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install \"flowgrad[all] @ git+https://github.com/hw01931/FlowGrad.git\" -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "print(\"âœ… Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. ðŸ”§ sklearn â€” GradientBoosting Tracking\n",
                "\n",
                "**Dataset**: Breast Cancer (569 samples, 30 features, binary classification)\n",
                "\n",
                "Track feature importance evolution using `warm_start` â€” every 10 rounds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "from flowgrad import SklearnTracker\n",
                "\n",
                "data = load_breast_cancer()\n",
                "X, y = data.data, data.target\n",
                "feature_names = list(data.feature_names)\n",
                "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "tracker = SklearnTracker(feature_names=feature_names)\n",
                "model = GradientBoostingClassifier(n_estimators=100, max_depth=3, warm_start=True, random_state=42)\n",
                "tracker.track_warm_start(model, X_train, y_train, X_val, y_val, step_size=10)\n",
                "\n",
                "print(tracker)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tracker.report();"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = tracker.plot.feature_drift(top_k=8)\n",
                "plt.show()\n",
                "\n",
                "fig = tracker.plot.feature_importance_heatmap(top_k=12)\n",
                "plt.show()\n",
                "\n",
                "fig = tracker.plot.overfitting_detector()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. ðŸŒ² RandomForest â€” Per-Tree Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "rf_model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
                "rf_model.fit(X_train, y_train)\n",
                "\n",
                "rf_tracker = SklearnTracker.from_forest(rf_model, feature_names=feature_names)\n",
                "print(f\"Trees: {rf_tracker.store.num_rounds}, Accuracy: {rf_model.score(X_val, y_val):.4f}\")\n",
                "\n",
                "fig = rf_tracker.plot.feature_drift(top_k=6)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. ðŸŒ² XGBoost â€” Round-by-Round Diagnostics\n",
                "\n",
                "**Dataset**: California Housing (20,640 samples, 8 features, regression)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import xgboost as xgb\n",
                "from flowgrad import BoostingTracker\n",
                "\n",
                "housing = fetch_california_housing()\n",
                "X_h, y_h = housing.data, housing.target\n",
                "feat_names_h = list(housing.feature_names)\n",
                "X_h_train, X_h_val, y_h_train, y_h_val = train_test_split(X_h, y_h, test_size=0.2, random_state=42)\n",
                "\n",
                "dtrain = xgb.DMatrix(X_h_train, label=y_h_train, feature_names=feat_names_h)\n",
                "dval = xgb.DMatrix(X_h_val, label=y_h_val, feature_names=feat_names_h)\n",
                "\n",
                "xgb_tracker = BoostingTracker()\n",
                "xgb_model = xgb.train(\n",
                "    {\"objective\": \"reg:squarederror\", \"max_depth\": 6, \"learning_rate\": 0.1, \"subsample\": 0.8},\n",
                "    dtrain, num_boost_round=200,\n",
                "    evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
                "    callbacks=[xgb_tracker.as_xgb_callback()],\n",
                "    verbose_eval=False,\n",
                ")\n",
                "\n",
                "xgb_tracker.report();"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = xgb_tracker.plot.eval_metrics()\n",
                "plt.show()\n",
                "\n",
                "fig = xgb_tracker.plot.feature_drift(top_k=8)\n",
                "plt.show()\n",
                "\n",
                "fig = xgb_tracker.plot.overfitting_detector()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. ðŸ§ª Feature Engineering â€” Interactions, Suggestions & VIF â­\n",
                "\n",
                "FlowGrad's key differentiator: **go beyond `df.corr()`**.\n",
                "\n",
                "- Detect non-linear feature synergies\n",
                "- Suggest concrete new features (A*B, A/B, etc.)\n",
                "- **NEW: VIF collinearity check** â€” flags suggestions that would cause multicollinearity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from flowgrad import FeatureAnalyzer\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "\n",
                "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42)\n",
                "rf_reg.fit(X_h_train, y_h_train)\n",
                "\n",
                "analyzer = FeatureAnalyzer(rf_reg, X_h_train, y_h_train, feature_names=feat_names_h)\n",
                "print(f\"Model RÂ² (val): {rf_reg.score(X_h_val, y_h_val):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Interactions â€” which pairs have synergy?\n",
                "interactions = analyzer.interactions(top_k=10, method=\"correlation\")\n",
                "print(\"\\nðŸ”— Top Feature Interactions:\")\n",
                "for i, item in enumerate(interactions, 1):\n",
                "    print(f\"  {i}. {item['feat_a']:>12s} Ã— {item['feat_b']:<12s}  synergy={item['synergy_score']:+.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Suggestions with VIF collinearity check\n",
                "suggestions = analyzer.suggest_features(top_k=15, collinearity_check=True, vif_threshold=10.0)\n",
                "print(\"\\nðŸ’¡ Suggested Features (with collinearity check):\")\n",
                "print(f\"   {'Expression':<35s} {'Corr':>6s} {'Lift':>6s} {'VIF':>8s} {'Warning'}\")\n",
                "print(\"   \" + \"â”€\" * 70)\n",
                "for item in suggestions:\n",
                "    if item['lift'] > 0:\n",
                "        vif = item.get('vif_score', '-')\n",
                "        warn = 'âš ï¸ HIGH' if item.get('collinearity_warning', False) else 'âœ… OK'\n",
                "        vif_str = f\"{vif:>7.1f}\" if isinstance(vif, float) else f\"{vif:>7s}\"\n",
                "        print(f\"   {item['expression']:<35s} {item['target_correlation']:>6.4f} {item['lift']:>+6.4f} {vif_str}  {warn}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Redundancy + Clustering\n",
                "redundant = analyzer.redundant_features(threshold=0.90)\n",
                "print(f\"\\nðŸ—‘ï¸ Redundant Features: {len(redundant)} pairs found\")\n",
                "for item in redundant[:5]:\n",
                "    print(f\"  {item['feat_a']} â†” {item['feat_b']}  corr={item['correlation']:.4f}  â†’ {item['recommendation']}\")\n",
                "\n",
                "clusters = analyzer.feature_clusters()\n",
                "print(f\"\\nðŸ“¦ Feature Clusters: {len(clusters)} clusters\")\n",
                "for c in clusters:\n",
                "    print(f\"  Cluster {c['cluster_id']} (cohesion={c['cohesion_score']:.2f}): {c['features']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualizations\n",
                "fig = analyzer.plot.interaction_heatmap(top_k=8)\n",
                "plt.show()\n",
                "\n",
                "fig = analyzer.plot.suggestion_chart(top_k=8)\n",
                "plt.show()\n",
                "\n",
                "fig = analyzer.plot.cluster_map()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. ðŸ“Š Before/After Comparison â€” Do Suggested Features Actually Help?\n",
                "\n",
                "Take the top suggested features (filtering out high-VIF ones), add them to the dataset, retrain, and compare."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "\n",
                "# Baseline model performance\n",
                "baseline_pred = rf_reg.predict(X_h_val)\n",
                "baseline_mse = mean_squared_error(y_h_val, baseline_pred)\n",
                "baseline_r2 = r2_score(y_h_val, baseline_pred)\n",
                "\n",
                "print(f\"ðŸ“Š BASELINE (original {X_h_train.shape[1]} features):\")\n",
                "print(f\"   MSE  = {baseline_mse:.4f}\")\n",
                "print(f\"   RÂ²   = {baseline_r2:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select top suggestions that are NOT collinear\n",
                "safe_suggestions = [s for s in suggestions \n",
                "                    if s['lift'] > 0 and not s.get('collinearity_warning', False)]\n",
                "top_safe = safe_suggestions[:5]\n",
                "\n",
                "print(f\"\\nâœ… Adding {len(top_safe)} safe features (low VIF):\")\n",
                "for s in top_safe:\n",
                "    print(f\"   + {s['expression']} (VIF={s.get('vif_score', 'N/A')})\")\n",
                "\n",
                "# Create augmented dataset\n",
                "def add_features(X, suggestions, feat_names):\n",
                "    X_aug = X.copy()\n",
                "    new_names = list(feat_names)\n",
                "    for s in suggestions:\n",
                "        i = feat_names.index(s['feat_a'])\n",
                "        j = feat_names.index(s['feat_b'])\n",
                "        op = s['operation']\n",
                "        if op == '*':\n",
                "            new_col = X[:, i] * X[:, j]\n",
                "        elif op == '/':\n",
                "            safe = np.where(np.abs(X[:, j]) > 1e-10, X[:, j], 1e-10)\n",
                "            new_col = X[:, i] / safe\n",
                "        elif op == '-':\n",
                "            new_col = X[:, i] - X[:, j]\n",
                "        elif op == '+':\n",
                "            new_col = X[:, i] + X[:, j]\n",
                "        else:\n",
                "            continue\n",
                "        X_aug = np.column_stack([X_aug, new_col])\n",
                "        new_names.append(s['expression'])\n",
                "    return X_aug, new_names\n",
                "\n",
                "X_train_aug, aug_names = add_features(X_h_train, top_safe, feat_names_h)\n",
                "X_val_aug, _ = add_features(X_h_val, top_safe, feat_names_h)\n",
                "\n",
                "print(f\"\\n   Features: {X_h_train.shape[1]} â†’ {X_train_aug.shape[1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train new model with augmented features\n",
                "rf_aug = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42)\n",
                "rf_aug.fit(X_train_aug, y_h_train)\n",
                "\n",
                "aug_pred = rf_aug.predict(X_val_aug)\n",
                "aug_mse = mean_squared_error(y_h_val, aug_pred)\n",
                "aug_r2 = r2_score(y_h_val, aug_pred)\n",
                "\n",
                "print(f\"\\n{'='*55}\")\n",
                "print(f\"  COMPARISON: Before vs After Feature Engineering\")\n",
                "print(f\"{'='*55}\")\n",
                "print(f\"  {'Metric':<10s} {'Baseline':>12s} {'+ Features':>12s} {'Change':>10s}\")\n",
                "print(f\"  {'â”€'*45}\")\n",
                "print(f\"  {'MSE':<10s} {baseline_mse:>12.4f} {aug_mse:>12.4f} {aug_mse-baseline_mse:>+10.4f}\")\n",
                "print(f\"  {'RÂ²':<10s} {baseline_r2:>12.4f} {aug_r2:>12.4f} {aug_r2-baseline_r2:>+10.4f}\")\n",
                "print(f\"  {'Features':<10s} {X_h_train.shape[1]:>12d} {X_train_aug.shape[1]:>12d} {X_train_aug.shape[1]-X_h_train.shape[1]:>+10d}\")\n",
                "\n",
                "if aug_r2 > baseline_r2:\n",
                "    print(f\"\\n  âœ… Feature engineering improved RÂ² by {(aug_r2-baseline_r2)*100:.2f}%!\")\n",
                "else:\n",
                "    print(f\"\\n  â„¹ï¸ No improvement. Consider different operations or domain-specific features.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. ðŸ”¬ PyTorch â€” DL Layer Health Tracking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from flowgrad import FlowTracker\n",
                "\n",
                "X_tensor = torch.FloatTensor(X_train)\n",
                "y_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
                "\n",
                "dl_model = nn.Sequential(\n",
                "    nn.Linear(30, 64), nn.ReLU(),\n",
                "    nn.Linear(64, 32), nn.ReLU(),\n",
                "    nn.Linear(32, 16), nn.ReLU(),\n",
                "    nn.Linear(16, 1), nn.Sigmoid(),\n",
                ")\n",
                "\n",
                "criterion = nn.BCELoss()\n",
                "optimizer = optim.Adam(dl_model.parameters(), lr=0.001)\n",
                "\n",
                "tracker = FlowTracker(dl_model)\n",
                "\n",
                "for epoch in range(50):\n",
                "    optimizer.zero_grad()\n",
                "    output = dl_model(X_tensor)\n",
                "    loss = criterion(output, y_tensor)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    tracker.step(loss=loss.item())\n",
                "\n",
                "print(f\"Final loss: {loss.item():.4f}\")\n",
                "tracker.report();"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = tracker.plot.loss()\n",
                "plt.show()\n",
                "\n",
                "fig = tracker.plot.velocity_heatmap()\n",
                "plt.show()\n",
                "\n",
                "fig = tracker.plot.health_dashboard()\n",
                "plt.show()\n",
                "\n",
                "fig = tracker.plot.full_report()\n",
                "plt.show()\n",
                "\n",
                "tracker.detach()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. ðŸ—œï¸ Model Compression â€” Auto Pruning Search & Layer Sensitivity â­\n",
                "\n",
                "**FlowGrad v0.3's flagship feature.**\n",
                "\n",
                "Set a performance floor â†’ FlowGrad automatically finds the maximum compression.\n",
                "\n",
                "```\n",
                "\"Keep 95% accuracy, compress as much as possible.\"\n",
                "â†’ FlowGrad: \"Sparsity 62% is optimal. Size reduced 62%, accuracy 97.3% retained.\"\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from flowgrad import CompressionTracker\n",
                "\n",
                "# Build and train a model for compression experiments\n",
                "comp_model = nn.Sequential(\n",
                "    nn.Linear(30, 128), nn.ReLU(),\n",
                "    nn.Linear(128, 64), nn.ReLU(),\n",
                "    nn.Linear(64, 32), nn.ReLU(),\n",
                "    nn.Linear(32, 1), nn.Sigmoid(),\n",
                ")\n",
                "\n",
                "criterion = nn.BCELoss()\n",
                "optimizer = optim.Adam(comp_model.parameters(), lr=0.001)\n",
                "\n",
                "for epoch in range(80):\n",
                "    optimizer.zero_grad()\n",
                "    output = comp_model(X_tensor)\n",
                "    loss = criterion(output, y_tensor)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "print(f\"Trained model | Loss: {loss.item():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create evaluation function\n",
                "X_val_tensor = torch.FloatTensor(X_val)\n",
                "y_val_tensor = torch.FloatTensor(y_val)\n",
                "\n",
                "def eval_fn(model):\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        preds = model(X_val_tensor).squeeze()\n",
                "        pred_labels = (preds > 0.5).float()\n",
                "        acc = (pred_labels == y_val_tensor).float().mean().item()\n",
                "    model.train()\n",
                "    return acc\n",
                "\n",
                "print(f\"Original accuracy: {eval_fn(comp_model):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ðŸŽ¯ Auto Compress â€” Goal-based search\n",
                "# \"Keep at least 90% of original accuracy, find max compression\"\n",
                "comp_tracker = CompressionTracker(comp_model, eval_fn=eval_fn)\n",
                "\n",
                "result = comp_tracker.auto_compress(\n",
                "    method=\"pruning\",\n",
                "    performance_floor=0.90,      # Keep 90%+ of original accuracy\n",
                "    search_range=(0.1, 0.9),     # Search sparsity from 10% to 90%\n",
                "    search_strategy=\"binary\",    # Binary search (fast)\n",
                "    precision=0.05,              # Stop when range < 5%\n",
                ")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"  Auto-Compress Result\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"  Original accuracy:  {result.performance_original:.4f}\")\n",
                "print(f\"  Compressed accuracy: {result.performance_compressed:.4f}\")\n",
                "print(f\"  Performance retained: {result.performance_retained*100:.1f}%\")\n",
                "print(f\"  Size: {result.size_original_mb:.2f}MB â†’ {result.size_compressed_mb:.2f}MB ({result.size_reduction*100:.1f}% reduction)\")\n",
                "print(f\"  Snapshots tested: {len(result.all_snapshots)}\")\n",
                "print(f\"\\n  ðŸ’Š {result.recommendation}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ðŸ“Š Compression Report\n",
                "comp_tracker.report();"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ðŸ“ˆ Tradeoff Curve â€” Pareto frontier of size vs performance\n",
                "fig = comp_tracker.plot.tradeoff_curve()\n",
                "plt.show()\n",
                "\n",
                "# ðŸ“Š Snapshot Comparison Timeline\n",
                "fig = comp_tracker.plot.compression_timeline()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ðŸ”¬ Layer Sensitivity â€” which layers can be aggressively pruned?\n",
                "print(\"Profiling layer sensitivity (this may take a moment)...\")\n",
                "sensitivity = comp_tracker.layer_sensitivity(\n",
                "    sparsity_levels=[0.1, 0.3, 0.5, 0.7, 0.9]\n",
                ")\n",
                "\n",
                "print(\"\\nðŸ”¬ Layer Sensitivity Results:\")\n",
                "baseline = eval_fn(comp_model)\n",
                "for layer_name, results in sensitivity.items():\n",
                "    print(f\"\\n  {layer_name}:\")\n",
                "    for sparsity, score in results:\n",
                "        drop = (baseline - score) / baseline * 100\n",
                "        emoji = \"ðŸ”´\" if drop > 5 else \"ðŸŸ¡\" if drop > 2 else \"ðŸŸ¢\"\n",
                "        print(f\"    {emoji} {sparsity:.0%} pruning â†’ score={score:.4f} (drop={drop:.1f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ðŸ”¥ Layer Sensitivity Heatmap\n",
                "fig = comp_tracker.plot.layer_sensitivity_heatmap()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ðŸ’Š Non-uniform pruning recommendation\n",
                "rec = comp_tracker.recommend_nonuniform(performance_floor=0.95)\n",
                "\n",
                "print(\"\\nðŸ’Š Non-Uniform Pruning Recommendation:\")\n",
                "print(\"  (Prune sensitive layers less, safe layers more)\\n\")\n",
                "for layer, sparsity in rec.items():\n",
                "    label = \"AGGRESSIVE\" if sparsity >= 0.7 else \"MODERATE\" if sparsity >= 0.3 else \"CONSERVATIVE\"\n",
                "    emoji = \"ðŸŸ¢\" if sparsity >= 0.5 else \"ðŸŸ¡\" if sparsity >= 0.3 else \"ðŸ”´\"\n",
                "    print(f\"  {emoji} {layer:<25s} â†’ {sparsity:.0%} pruning ({label})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "| Feature | Code |\n",
                "|---|---|\n",
                "| DL Tracking | `FlowTracker(model)` â†’ `.step(loss=...)` |\n",
                "| XGBoost | `callbacks=[tracker.as_xgb_callback()]` |\n",
                "| sklearn | `tracker.track_warm_start(model, X, y)` |\n",
                "| Feature Engineering | `FeatureAnalyzer(model, X, y)` |\n",
                "| VIF Check | `.suggest_features(collinearity_check=True)` |\n",
                "| **Compression** | `CompressionTracker(model, eval_fn)` |\n",
                "| **Auto Search** | `.auto_compress(performance_floor=0.95)` |\n",
                "| **Layer Sensitivity** | `.layer_sensitivity()` |\n",
                "| Report | `.report()` |\n",
                "| Plots | `.plot.full_report()` |\n",
                "\n",
                "**GitHub**: [https://github.com/hw01931/FlowGrad](https://github.com/hw01931/FlowGrad)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}