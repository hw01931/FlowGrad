# FlowGrad ğŸŒŠ

**ML í•™ìŠµ ê³¼ì • ì§„ë‹¨ + í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬** â€” ì½”ë“œ í•œ ì¤„ë¡œ ëª¨ë¸ í•™ìŠµ ì—­í•™ì„ ì¶”ì í•˜ê³ , í”¼ì²˜ ìƒí˜¸ì‘ìš©ì„ ë¶„ì„í•©ë‹ˆë‹¤.

## âœ¨ Features

- ğŸ”¬ **PyTorch**: ë ˆì´ì–´ë³„ ê°€ì¤‘ì¹˜ ì†ë„Â·ê°€ì†ë„Â·ê±´ê°• ìƒíƒœ ìë™ ì¶”ì 
- ğŸŒ² **XGBoost / LightGBM / CatBoost**: ë¼ìš´ë“œë³„ í”¼ì²˜ ì¤‘ìš”ë„ ë³€í™”Â·ê³¼ì í•© íƒì§€
- ï¿½ **scikit-learn**: GradientBoosting(warm_start), RandomForest(per-tree), SGD(partial_fit) ì§€ì›
- ğŸ§ª **Feature Engineering**: í”¼ì²˜ ìƒí˜¸ì‘ìš©Â·ì¡°í•© ì œì•ˆÂ·ì¤‘ë³µ íƒì§€Â·í´ëŸ¬ìŠ¤í„°ë§
- ï¿½ğŸ“Š **ì‹œê°í™”**: ë‹¤í¬ í…Œë§ˆ ëŒ€ì‹œë³´ë“œ, íˆíŠ¸ë§µ, SNR ì°¨íŠ¸ ë“± 15+ ì°¨íŠ¸
- ğŸ’Š **ìë™ ì§„ë‹¨**: ì •ì²´Â·í­ì£¼Â·ê³¼ì í•© íƒì§€ + í…ìŠ¤íŠ¸ ì²˜ë°©

## Quick Start

### Installation

```bash
pip install -e ".[all]"
```

### 1. PyTorch â€” DL Training Tracker

```python
from flowgrad import FlowTracker

tracker = FlowTracker(model)

for epoch in range(100):
    loss = train_one_epoch(model, loader, optimizer)
    tracker.step(loss=loss.item())

tracker.report()                  # ì¢…í•© ì§„ë‹¨ ë¦¬í¬íŠ¸
tracker.plot.velocity_heatmap()   # ë ˆì´ì–´ë³„ í•™ìŠµ ì†ë„ íˆíŠ¸ë§µ
tracker.plot.health_dashboard()   # ë ˆì´ì–´ ê±´ê°• ìƒíƒœ
tracker.plot.full_report()        # ì¢…í•© ëŒ€ì‹œë³´ë“œ (6ê°œ ì°¨íŠ¸)
```

### 2. XGBoost / LightGBM / CatBoost

```python
from flowgrad import BoostingTracker
import xgboost as xgb

tracker = BoostingTracker()
model = xgb.train(params, dtrain, num_boost_round=500,
                  evals=[(dtrain, "train"), (dvalid, "valid")],
                  callbacks=[tracker.as_xgb_callback()])

tracker.report()
tracker.plot.feature_drift()              # í”¼ì²˜ ì¤‘ìš”ë„ ë³€í™”
tracker.plot.overfitting_detector()       # ê³¼ì í•© íƒì§€
```

```python
# LightGBM
tracker = BoostingTracker()
model = lgb.train(params, dtrain, callbacks=[tracker.as_lgb_callback()])

# CatBoost
tracker = BoostingTracker()
model = CatBoostClassifier(iterations=500)
model.fit(X, y, callbacks=[tracker.as_catboost_callback()])
```

### 3. scikit-learn

```python
from flowgrad import SklearnTracker
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier

# GradientBoosting (warm_start ë°©ì‹ìœ¼ë¡œ ë¼ìš´ë“œë³„ ì¶”ì )
tracker = SklearnTracker(feature_names=feature_names)
model = GradientBoostingClassifier(n_estimators=200, warm_start=True)
tracker.track_warm_start(model, X_train, y_train, X_val, y_val, step_size=10)
tracker.report()

# RandomForest (ê°œë³„ íŠ¸ë¦¬ ë¶„ì„)
model = RandomForestClassifier(n_estimators=100).fit(X, y)
tracker = SklearnTracker.from_forest(model, feature_names=feature_names)
tracker.plot.feature_drift()  # íŠ¸ë¦¬ë³„ í”¼ì²˜ ì¤‘ìš”ë„ ë³€í™”

# SGDClassifier (partial_fit ë°°ì¹˜ë³„ ì¶”ì )
tracker = SklearnTracker()
tracker.track_partial_fit(model, X_batches, y_batches, classes=[0, 1])
```

### 4. Feature Engineering â­ (ì°¨ë³„í™” ê¸°ëŠ¥)

```python
from flowgrad import FeatureAnalyzer

analyzer = FeatureAnalyzer(model, X_train, y_train, feature_names=feature_names)

# í”¼ì²˜ ìƒí˜¸ì‘ìš© ë¶„ì„ (ê¸°ì¡´ corr()ê³¼ ë‹¤ë¥´ê²Œ ë¹„ì„ í˜• ì‹œë„ˆì§€ ì¸¡ì •)
interactions = analyzer.interactions(top_k=10)
# â†’ [{"feat_a": "age", "feat_b": "income", "synergy_score": 0.12}, ...]

# í”¼ì²˜ ì¡°í•© ì œì•ˆ (A*B, A/B ë“± ìë™ í…ŒìŠ¤íŠ¸)
suggestions = analyzer.suggest_features(top_k=10)
# â†’ [{"expression": "age * income", "lift": 0.08, "target_correlation": 0.72}, ...]

# ì¤‘ë³µ í”¼ì²˜ íƒì§€
redundant = analyzer.redundant_features(threshold=0.95)
# â†’ [{"feat_a": "height_cm", "feat_b": "height_inch", "recommendation": "Drop height_inch"}]

# í”¼ì²˜ í´ëŸ¬ìŠ¤í„°ë§
clusters = analyzer.feature_clusters()
# â†’ [{"cluster_id": 0, "features": ["age", "income"], "cohesion": 0.85}, ...]

# ì¢…í•© ë¦¬í¬íŠ¸
analyzer.report()

# ì‹œê°í™”
analyzer.plot.interaction_heatmap()   # ìƒí˜¸ì‘ìš© íˆíŠ¸ë§µ
analyzer.plot.suggestion_chart()      # ì¡°í•© ì œì•ˆ ì°¨íŠ¸
analyzer.plot.redundancy_graph()      # ì¤‘ë³µ ë„¤íŠ¸ì›Œí¬
analyzer.plot.cluster_map()           # í´ëŸ¬ìŠ¤í„° ë§µ
```

## ê¸°ì¡´ ë„êµ¬ì™€ì˜ ì°¨ì´ì 

| ê¸°ì¡´ | FlowGrad |
|---|---|
| `df.corr()` | ì„ í˜• ìƒê´€ë§Œ | **ë¹„ì„ í˜• ìƒí˜¸ì‘ìš© + ì‹œë„ˆì§€** ì¸¡ì • |
| `model.feature_importances_` | í•™ìŠµ ëë‚œ í›„ ê²°ê³¼ë¡ ì  | **í•™ìŠµ ì¤‘** ì‹¤ì‹œê°„ ë³€í™” ì¶”ì  |
| SHAP | "ì™œ ì´ ì˜ˆì¸¡?" (ê²°ê³¼ í•´ì„) | **ì–´ë–¤ í”¼ì²˜ë¥¼ ë§Œë“¤ë©´ ì¢‹ì„ì§€** ì œì•ˆ |
| TensorBoard | ìˆ˜ë™ ë¡œê¹… í•„ìš” | **í•œ ì¤„**ì´ë©´ ì „ì²´ ì¶”ì  ì‹œì‘ |

## Available Plots

### DL (PyTorch) â€” 7 charts
`loss()` Â· `velocity_heatmap()` Â· `gradient_flow()` Â· `weight_distribution()` Â· `health_dashboard()` Â· `gradient_snr()` Â· `full_report()`

### Boosting / sklearn â€” 5 charts
`eval_metrics()` Â· `feature_drift()` Â· `feature_importance_heatmap()` Â· `overfitting_detector()` Â· `full_report()`

### Feature Engineering â€” 4 charts
`interaction_heatmap()` Â· `suggestion_chart()` Â· `redundancy_graph()` Â· `cluster_map()`

## License

MIT
